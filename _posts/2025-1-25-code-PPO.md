---
title: Proximal Policy Optimization
categories: [code]
comments: true
---


```python
import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import torch
import torch.optim as optim
from torch.distributions import Categorical
from copy import deepcopy

#Hyperparameters
LEARNING_RATE=0.0005
GAMMA=0.95
LAMBDA=0.95
N_STEP=1
EPSILON=0.1
EPOCH=10
```


```python
class Agent(nn.Module):
    def __init__(self, Dim_in=2*2, Dim_out=2, hiddens=[128,128,128]):
        super(Agent, self).__init__()
       
        #data
        self.s_list = []
        self.a_list = []
        self.r_list = []
        self.s_prime_list = []
        self.done_list = []
        
        #Dim
        self.Dim_in = Dim_in
        self.Dim_out = Dim_out
        if(type(hiddens)==list):
            self.hiddens = hiddens
        else: print('the variable hiddens is not list')
        
        
        self.fc= nn.ModuleList()
        #
        #State x Action -> R^n
        inputSize = self.Dim_in 
        for nextSize in self.hiddens:
            self.fc.append( nn.Linear(inputSize, nextSize, bias=True) )
            inputSize= nextSize
        
        #Critic Net: R^n -> R (Value)
        self.layer_Val= nn.Linear(inputSize, 1)
        
        #Act Net: R^n -> 
        self.layer_Act= nn.Linear(inputSize, self.Dim_out) 
        
        #Optimizer
        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)
    
          
    def print_param(self):
        for layer in self.fc:
            generator=layer.parameters()
            while True:
                try:
                    w = next(generator)
                    print(w.data)
                    
                except StopIteration:
                    break    
```


```python
def forward(self, state):
    inputs_= state
    for i in range( len(self.hiddens) ):
        outputs_ = self.fc[i](inputs_)
        inputs_ = F.relu(outputs_)
    return inputs_

def getValue(self,state):
    inputs_= self.forward(state)
    output= self.layer_Val(inputs_)
    return output.view(-1,1)

def getActionProb_(self, state):
    inputs_= self.forward(state)
    output= self.layer_Act(inputs_)
    #print(output.shape)
    
    prob= F.softmax(output, dim=-1).view(-1,self.Dim_out)
    #output= torch.clamp(output, min=-0.5, max=0.5)
    return prob





Agent.forward = forward
Agent.getValue = getValue

Agent.getActionProb = getActionProb_

```


```python
def train(self, p_olds):
    s, a, r, s_prime, done = self.makeBatch()
    
    for i in range(EPOCH):
        v_hat= self.getValue(s)
        v_prime_hat= self.getValue(s_prime)

        #If done=0 (terminal state), then the episode is stopped & end
        td_target= r + GAMMA*v_prime_hat*done 
        delta = td_target-v_hat
        delta = delta.detach()

    #     advantages=[]
    #     advantage=0
    #     for delta_t in delta[::-1]:
    #         advantage= GAMMA*LAMBDA*advantage + delta_t[0]
    #         advantages.append(advantage)
    #     advantages.reverse()
    #     GAE=torch.tensor(advantages)

        p=self.getActionProb(s)
        p_a=p.gather(-1,a)

        ratio=  torch.log(p_a / p_olds )

        surr1 = ratio * delta
        surr2 = ratio.clip(1-EPSILON,1+EPSILON) * delta
        loss= - torch.min(surr1,surr2) + F.smooth_l1_loss(v_hat, td_target.detach())

        self.optimizer.zero_grad()
        loss.mean().backward()
        self.optimizer.step()

Agent.train= train
```


```python
def putData(self, transition=()):
    assert type(transition) is tuple, print(f"must put tuple not {type(transition)}")
    
    self.s_list.append(transition[0]) 
    self.a_list.append(transition[1]) 
    self.r_list.append(transition[2]) 
    self.s_prime_list.append(transition[3]) 
    self.done_list.append(transition[4]) 
    #print(len(transition[0]),len(transition[1]),len(transition[2]),len(transition[3]),len(transition[4]))
    
def makeBatch(self):
    s_batch = torch.tensor(self.s_list).view(-1,self.Dim_in)
    a_batch = torch.tensor(self.a_list).view(-1,1)
    r_batch = torch.tensor(self.r_list).view(-1,1)
    s_prime_batch = torch.tensor(self.s_prime_list).view(-1,self.Dim_in)
    #
    done_arr=np.where(self.done_list, 0, 1)
    done_batch = torch.torch.from_numpy(done_arr).view(-1,1)
    
    self.s_list = []
    self.a_list = []
    self.r_list = []
    self.s_prime_list = []
    self.done_list = []
    #print(r_batch, done_batch)
    return (s_batch, a_batch, r_batch, s_prime_batch, done_batch)

Agent.putData= putData
Agent.makeBatch= makeBatch

```

## Example Cartpole
Note, the model is reward-sensitive & Hyperparameter sensitive



```python
import gymnasium as gym

def do_simulation(MaxEpisode=300, n=30 ,EnvName="CartPole-v1"):
    env = gym.make(EnvName, render_mode="rgb_array")
    s_size = env.observation_space.shape[0]
    a_size = env.action_space.n

    agent=Agent(Dim_in=s_size, Dim_out=a_size)
    
    
    #agent.print_param()
    scoreHistory=[]
    sumScore=0
    score=0.
    for episode in range(MaxEpisode):
        done=False
        truncated=False
        s= env.reset()[0]
        
        p_old=[]
        while not (done or truncated):
            for _ in range(N_STEP):
                p= agent.getActionProb( torch.from_numpy(s).float() )
                dist= Categorical(probs=p)
                act= dist.sample().item()
                #act=torch.argmax(p).item()
                
                p_old.append(p[0][act])
                
                #Act
                s_prime, r, done, truncated, info = env.step(act)

                #Note, the model is reward-sensitive
                agent.putData((s,act,r/100.,s_prime,done))
                
                #goal to the next state & get the reward in the state
                s=s_prime
                score += r

                if done or truncated: 
                    break
        p_old=torch.tensor(p_old)
        
        
        agent.train(p_old)
         
        
        
        if(episode>=100):
            sumScore-= scoreHistory[episode-100]
        sumScore+=score
        scoreHistory.append(score)
        score=0
        #calc fitness
        #fitness[winner_idx]= score
        #rank=np.argsort(fitness) 
        
        
        ##End Condition
        if(sumScore>500*100):
            env.close()
            print(f'Learning end at {episode}')
            return agent, scoreHistory, episode
        
        
        
        if((episode+1) % n == 0):
            print(f'average score for {n}-times:',end='')
            print(np.mean(scoreHistory[-n:]))
            
    env.close()
    #print('Fail to learn')

    return agent, scoreHistory, MaxEpisode
```


```python
 agent, scoreHistory, MaxEpisode = do_simulation()
```

    average score for 30-times:42.8
    average score for 30-times:117.46666666666667
    average score for 30-times:276.0
    average score for 30-times:255.96666666666667
    average score for 30-times:443.76666666666665
    average score for 30-times:487.93333333333334
    average score for 30-times:481.73333333333335
    average score for 30-times:475.8333333333333
    average score for 30-times:485.03333333333336
    average score for 30-times:494.8
    


```python
# Test cartpole
import matplotlib.pyplot as plt
import imageio
imgs=[]


env = gym.make("CartPole-v1", render_mode="rgb_array")


total_rewards = 0
state = env.reset()[0]
done, truncation = False, False

while not (done or truncation):
    p= agent.getActionProb( torch.from_numpy(state).float() )
    dist= Categorical(probs=p)
    action= dist.sample().item()
    
    new_state,reward,done,truncation ,_= env.step(action)
    total_rewards += reward
    state = new_state
    
    #env.render()
    img = (env.render())
    imgs.append(img)
print(total_rewards)
env.close()

imageio.mimsave('cartpole1.gif', imgs, format='GIF', duration=0.001)
```

    500.0
    


```python

```
