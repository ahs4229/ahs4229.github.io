---
title: DQN
categories: [code]
comments: true
---

```python
import gymnasium as gym
import torch
import torch.nn as nn
import torch.nn.functional as F

import numpy as np
from collections import deque, namedtuple
import random

```


```python
!python --version
print("Library numpy:", np.__version__)
print("Library torch:",torch.__version__)

print("Library gym:",gym.__version__)
```

    Python 3.8.5
    Library numpy: 1.24.4
    Library torch: 2.4.1+cpu
    Library gym: 1.1.1
    


```python
env = gym.make("CartPole-v1", render_mode="rgb_array")

class QNet(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(QNet, self).__init__()
        
        #state->hidden->Q  -> action
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, a_size)
        
    # Forward Propagation
    def forward(self, x):        
        x = torch.Tensor(x)
        
        x = F.relu(self.fc1(x))
        #print( x.shape)
        x = self.fc2(x)
        #print( x.shape)
        return x
    
    # Select the best Q-value and the corresponding action
    def act(self, state):
        pred = self.forward(state)
        #print( pred)
        
        #the resulted action is the best action at the presented state given model weigts
        result = pred.squeeze().argmax().item()
        return result

net_test = QNet(4,2,64)
state = env.reset()[0].reshape(1,4)
net_test.forward( env.reset()[0].reshape(1,4) )
net_test.act(state)

```




    0




```python
Sequence = namedtuple("Sequence",('state','action','reward','next_state','done'))

class ReplayMemory:
    def __init__(self, capacity):
        self.capacity = capacity        
        self.memory = deque(maxlen=self.capacity)

    def push(self, *args):
        self.memory.append(Sequence(*args))
    
    def sample(self, batch_size):
        experiences = random.sample(self.memory, batch_size)
        return Sequence(*zip(*experiences))

    
    
    
done, truncation = False, False
memory_test = ReplayMemory(500)
state = env.reset()[0]

while not (done or truncation):
    action = env.action_space.sample()
    next_state,reward,done,truncation,_ = env.step(action)
    memory_test.push(state,action,reward,next_state,done)
    state = next_state
    
batch = memory_test.sample(5)
print(batch.state)
```

    (array([ 0.03823864, -0.18199073, -0.00944336,  0.31343344], dtype=float32), array([-0.06477235, -0.9689071 ,  0.159168  ,  1.6428425 ], dtype=float32), array([-0.08415049, -0.7759668 ,  0.19202486,  1.4036884 ], dtype=float32), array([-0.00337367, -0.37796557,  0.05700096,  0.6257135 ], dtype=float32), array([ 0.04162616, -0.18236344, -0.01635516,  0.32169273], dtype=float32))
    


```python
def calc_loss(batch, net, GAMMA):
    s = torch.FloatTensor(batch.state)
    
    r = torch.Tensor(batch.reward)    
    act = torch.Tensor(batch.action).to(torch.int64)    
    next_s = torch.FloatTensor(batch.next_state)
    m = 1-torch.Tensor(batch.done).to(torch.int64)

    target = r + GAMMA * net.forward(next_s).amax(1) * m
    target = target.view(1,-1)
    
    pred = net.forward(s).gather(1,act.view(-1,1)).view(1,-1)    
    
    ## COMPLETE THIS LINE
    loss = (1/len(batch))*torch.sum( (target-pred)**2 )
    
    return loss

```


```python
def dqn_replay_memory(GAMMA, EPS_START, EPS_END, EPS_DECAY, NUM_EPI, BATCH_SIZE):
    # initialize replay memory
    memory = ReplayMemory(1000) 
    
    # initialize Q-network
    net = QNet(4,2,32)

    optimizer = torch.optim.Adam(net.parameters())
    
    # for episodes 1,M do
    avg_rewards = deque(maxlen=50)

    for epi in range(NUM_EPI+1):
        # init sequence
        state = env.reset()[0]
        done, truncation = False, False
        reward_list = []

        # iterate inside the sequence
        # calculate EPS
        EPS = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * epi / EPS_DECAY)
        while not (done or truncation):
            with torch.no_grad():
            # with prob EPS select random action
                if np.random.random() < EPS:
                    action = env.action_space.sample()
                else: # otherwise select action w.r.t policy
                    action = net.act(state) # Complete this code
                
            # proceed
            next_state, reward, done, truncation, _ = env.step(action)
            reward_list.append(reward)
            
            # store transition in memory
            memory.push(state,action,reward,next_state,done) # Complete this code
            
            # sample random minibatch from memory
            if len(memory.memory)<BATCH_SIZE:
                continue
            else:#len(memory.memory)==BATCH_SIZE
                batch =  memory.sample(BATCH_SIZE)
                
            # calculate loss
            loss=calc_loss(batch, net, GAMMA)

            # perform gradient descent step
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # update state
            state = next_state
            
        # insert avg
        avg_rewards.append(sum(reward_list))
        if epi%50 == 0:
            print(epi, np.mean(avg_rewards), EPS)

    return net


```


```python
#hyperparameter
discount_rate = 0.95
eps_start = 0.99
eps_end = 0.02
eps_decay = 200
num_epi = 500
batch_size =128


#do
result = dqn_replay_memory(   
    discount_rate, eps_start, eps_end, eps_decay, num_epi, batch_size)
```

    0 26.0 0.99
    

    <ipython-input-5-4f0e77b2ed51>:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\utils\tensor_new.cpp:281.)
      s = torch.FloatTensor(batch.state)
    

    50 23.14 0.7754367595792627
    100 29.06 0.6083347399212544
    150 46.8 0.47819555615878423
    200 162.76 0.3768430579362991
    250 332.38 0.2979096529543844
    300 365.22 0.23643625534397691
    350 205.7 0.18856072514693178
    400 175.16 0.1512752247395143
    450 273.62 0.1222372478250084
    500 235.76 0.09962244866518184
    


```python
# Test cartpole
import matplotlib.pyplot as plt
import imageio
imgs=[]

total_rewards = 0
state = env.reset()[0]
done, truncation = False, False

while not (done or truncation):
    action = result.act(state)    
    new_state,reward,done,truncation ,_= env.step(action)
    total_rewards += reward
    state = new_state
    #env.render()
    img = (env.render())
    imgs.append(img)
print(total_rewards)
env.close()

imageio.mimsave('cartpole1.gif', imgs, format='GIF', duration=0.001)
```

    500.0
    


```python

```
