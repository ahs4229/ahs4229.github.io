---

title: Multi-layerd Perceptron 

categories: \[code]

comments: true

---





```python

import numpy as np

import matplotlib.pyplot as plt

import random

from copy import deepcopy

```





```python

def Leaky\_ReLU(x):

&nbsp;   return np.maximum(x,-0.001\*x)



def derivative\_Leack\_ReLU(x):

&nbsp;   if(x<0):

&nbsp;       return -0.001

&nbsp;   else: 

&nbsp;       return 1

```





```python

LEARNING\_RATE=0.00001



class Identifier:

&nbsp;   def \_\_init\_\_(self, Dim\_in=2\*2, Dim\_out=2, hiddens=\[128,128], l=0.001):

&nbsp;       '''

&nbsp;       Identifier: S x A → S

&nbsp;       '''

&nbsp;       self.Lambda=l

&nbsp;       



&nbsp;       #Initialization

&nbsp;       self.Dim\_in= Dim\_in

&nbsp;       self.Dim\_out= Dim\_out

&nbsp;       self.hiddens = hiddens

&nbsp;       self.Depth= len(hiddens)+1

&nbsp;       

&nbsp;       

&nbsp;       self.weights= \[]

&nbsp;       self.biases= \[]

&nbsp;       

&nbsp;       #State X Action -> R^n

&nbsp;       inputSize = self.Dim\_in 

&nbsp;       for nextSize in self.hiddens:

&nbsp;           self.weights.append( 2\*np.random.rand(inputSize, nextSize) -1 )

&nbsp;           self.biases.append( 2\*np.random.rand(1, nextSize) -1)

&nbsp;           inputSize= nextSize

&nbsp;           

&nbsp;       #Identifier Net: R^n -> State

&nbsp;       self.identifier= 2\*np.random.rand(inputSize, self.Dim\_out) -1



```





```python

def forward(self, inputs):

&nbsp;   inputs= np.array(inputs).reshape(-1,self.Dim\_in)

&nbsp;   

&nbsp;   global inputList 

&nbsp;   inputList = \[inputs]

&nbsp;   

&nbsp;   for i in range( len(self.hiddens) ):

&nbsp;       outputs = (inputs)@self.weights\[i] + self.biases\[i]

&nbsp;       inputs =  Leaky\_ReLU(outputs)

&nbsp;       inputList.append(inputs)

&nbsp;   s\_prime\_hat = (inputs)@self.identifier   

&nbsp;   return s\_prime\_hat

&nbsp;

&nbsp;   

def train(self, inputs=\[], targets=\[], Loss="MSE"):

&nbsp;   #Data= \[s]+\[a] 

&nbsp;   inputs= np.array(inputs).reshape(-1,self.Dim\_in) 

&nbsp;   s\_prime= np.array(targets).reshape(-1,self.Dim\_out) 

&nbsp;   

&nbsp;   

&nbsp;   ##forward propagation

&nbsp;   s\_prime\_hat= self.forward(inputs)  

&nbsp;   

&nbsp;   

\#     l2= np.sum(self.identifier)

\#         for weight in self.weights:

\#             l2+=np.sum(weight)

\#     l2 \*= self.Lambda 



&nbsp;   NM=len(inputs)

&nbsp;   ##Loss calculation

&nbsp;   def get\_Loss(s\_prime,s\_prime\_hat, Loss=""):

&nbsp;       if(Loss=="MSE"):

&nbsp;           deviation= s\_prime-s\_prime\_hat

&nbsp;           L = (deviation\*\*2)/2/NM

&nbsp;           dL\_dhat = -deviation/NM



&nbsp;       elif(Loss== "Newsvendor"):

&nbsp;           #s\_prime: real demand

&nbsp;           #s\_prime\_hat: invetory

&nbsp;           L = (2\*np.max(s\_prime-s\_prime\_hat,0) + 1\*np.max(s\_prime\_hat-s\_prime,0)) / NM

&nbsp;           dL\_dhat = np.where(s\_prime-s\_prime\_hat>0, -2, 1)

&nbsp;       

&nbsp;       return L, dL\_dhat

&nbsp;   

&nbsp;   

&nbsp;   

&nbsp;   L, dL\_dhat = get\_Loss(s\_prime=s\_prime,s\_prime\_hat=s\_prime\_hat, Loss=Loss)

&nbsp;   

&nbsp;   ##Backpropagation    

&nbsp;   self.backpropagation(dL\_dhat)

&nbsp;   

&nbsp;   

&nbsp;   return np.mean(L)

&nbsp;                                             

&nbsp;       

def backpropagation(self, dL\_dhat):

&nbsp;   dhat\_dW = inputList\[self.Depth-1].T

&nbsp;   #

&nbsp;   presWeight = (self.identifier).T

&nbsp;   self.identifier -= LEARNING\_RATE\* dhat\_dW@dL\_dhat + self.Lambda\*self.identifier

&nbsp;   

&nbsp;   dL\_dout = (dL\_dhat\*1)@presWeight

&nbsp;   for i in range(0, self.Depth-1, -1):

&nbsp;       dout\_dW = inputList\[i].T #derivative\_Leack\_ReLU(inputList\[i+1]

&nbsp;       dout\_dB = 1\*derivative\_Leack\_ReLU(inputList\[i+1])

&nbsp;       #

&nbsp;       presWeight = self.weights\[i].T

&nbsp;       self.weights\[i] -= LEARNING\_RATE\* dout\_dW@(dL\_dout\*derivative\_Leack\_ReLU(inputList\[i+1])) + self.Lambda \* self.weights\[i]

&nbsp;       self.biases\[i] -=  LEARNING\_RATE\* dout\_dB\*(dL\_dout)

&nbsp;       

&nbsp;       dL\_dout= (dL\_dout\*derivative\_Leack\_ReLU(inputList\[i+1]))@presWeight

&nbsp;                                         

&nbsp;   return None



&nbsp;                                         



Identifier.forward=forward

Identifier.train=train

Identifier.backpropagation=backpropagation

&nbsp;                                       

```



Overflow maybe arise ,when the Learning rate is sufficiently high





```python

def f(x):

&nbsp;   return x\*\*2







x=np.linspace(-10,10,100)

y=f(x)





LEARNING\_RATE=0.000000001

identifier = Identifier(Dim\_in=1, Dim\_out=1, hiddens=\[64,128,256,64,32])



for \_ in range(10000):

&nbsp;   identifier.train(inputs=x, targets=y)

mse = identifier.train(inputs=x, targets=y)



print(mse)

y\_hat=identifier.forward(x)

plt.plot(x,y\_hat,c='b',label='idenfifier')

plt.plot(x,y,c='r',label='target')

plt.legend()

plt.show()

```



&nbsp;   0.7174388287505724

&nbsp;   





&nbsp;   

!\[png](output\_5\_1.png)

&nbsp;   







```python

from scipy.stats import norm

from scipy.stats import ttest\_ind



def f(x):

&nbsp;   global w

&nbsp;   w=np.ones(10)/10

&nbsp;   y=np.random.normal(3+x@w)

&nbsp;   return y



costs=\[]

costs\_opt=\[]



for \_ in range(10):

&nbsp;   #demand data

&nbsp;   x=np.random.rand(100,10)

&nbsp;   y=f(x)



&nbsp;   #optimal cost

&nbsp;   y\_opt=norm.ppf(2/3, loc=3+x@w)

&nbsp;   cost\_opt= (2\*np.max(y-y\_opt,0) + 1\*np.max(y\_opt-y,0)) / 100

&nbsp;   costs\_opt.append(cost\_opt)

&nbsp;   

&nbsp;   #

&nbsp;   LEARNING\_RATE=0.000000001

&nbsp;   identifier = Identifier(Dim\_in=10, Dim\_out=1, hiddens=\[32,64,128,256,64,32])



&nbsp;   for \_ in range(10000):

&nbsp;       identifier.train(inputs=x, targets=y, Loss="Newsvendor")

&nbsp;   cost = identifier.train(inputs=x, targets=y, Loss="Newsvendor")

&nbsp;   costs.append(cost)



print(np.mean(costs))

print(np.mean(costs\_opt))



print(ttest\_ind(costs,costs\_opt,equal\_var=False))



plt.boxplot(\[costs,costs\_opt])

plt.xticks(\[1,2],\['deep','opt'])

plt.show()

```



&nbsp;   0.07442943501667457

&nbsp;   0.07062499773231493

&nbsp;   Ttest\_indResult(statistic=0.8206760282986177, pvalue=0.4239645263738895)

&nbsp;   





&nbsp;   

!\[png](output\_6\_1.png)

&nbsp;   







```python



```

```python

import numpy as np

import matplotlib.pyplot as plt

import random

from copy import deepcopy

```





```python

def Leaky\_ReLU(x):

&nbsp;   return np.maximum(x,-0.001\*x)



def derivative\_Leack\_ReLU(x):

&nbsp;   if(x<0):

&nbsp;       return -0.001

&nbsp;   else: 

&nbsp;       return 1

```





```python

LEARNING\_RATE=0.00001



class Identifier:

&nbsp;   def \_\_init\_\_(self, Dim\_in=2\*2, Dim\_out=2, hiddens=\[128,128], l=0.001):

&nbsp;       '''

&nbsp;       Identifier: S x A → S

&nbsp;       '''

&nbsp;       self.Lambda=l

&nbsp;       



&nbsp;       #Initialization

&nbsp;       self.Dim\_in= Dim\_in

&nbsp;       self.Dim\_out= Dim\_out

&nbsp;       self.hiddens = hiddens

&nbsp;       self.Depth= len(hiddens)+1

&nbsp;       

&nbsp;       

&nbsp;       self.weights= \[]

&nbsp;       self.biases= \[]

&nbsp;       

&nbsp;       #State X Action -> R^n

&nbsp;       inputSize = self.Dim\_in 

&nbsp;       for nextSize in self.hiddens:

&nbsp;           self.weights.append( 2\*np.random.rand(inputSize, nextSize) -1 )

&nbsp;           self.biases.append( 2\*np.random.rand(1, nextSize) -1)

&nbsp;           inputSize= nextSize

&nbsp;           

&nbsp;       #Identifier Net: R^n -> State

&nbsp;       self.identifier= 2\*np.random.rand(inputSize, self.Dim\_out) -1



```





```python

def forward(self, inputs):

&nbsp;   inputs= np.array(inputs).reshape(-1,self.Dim\_in)

&nbsp;   

&nbsp;   global inputList 

&nbsp;   inputList = \[inputs]

&nbsp;   

&nbsp;   for i in range( len(self.hiddens) ):

&nbsp;       outputs = (inputs)@self.weights\[i] + self.biases\[i]

&nbsp;       inputs =  Leaky\_ReLU(outputs)

&nbsp;       inputList.append(inputs)

&nbsp;   s\_prime\_hat = (inputs)@self.identifier   

&nbsp;   return s\_prime\_hat

&nbsp;

&nbsp;   

def train(self, inputs=\[], targets=\[], Loss="MSE"):

&nbsp;   #Data= \[s]+\[a] 

&nbsp;   inputs= np.array(inputs).reshape(-1,self.Dim\_in) 

&nbsp;   s\_prime= np.array(targets).reshape(-1,self.Dim\_out) 

&nbsp;   

&nbsp;   

&nbsp;   ##forward propagation

&nbsp;   s\_prime\_hat= self.forward(inputs)  

&nbsp;   

&nbsp;   

\#     l2= np.sum(self.identifier)

\#         for weight in self.weights:

\#             l2+=np.sum(weight)

\#     l2 \*= self.Lambda 



&nbsp;   NM=len(inputs)

&nbsp;   ##Loss calculation

&nbsp;   def get\_Loss(s\_prime,s\_prime\_hat, Loss=""):

&nbsp;       if(Loss=="MSE"):

&nbsp;           deviation= s\_prime-s\_prime\_hat

&nbsp;           L = (deviation\*\*2)/2/NM

&nbsp;           dL\_dhat = -deviation/NM



&nbsp;       elif(Loss== "Newsvendor"):

&nbsp;           #s\_prime: real demand

&nbsp;           #s\_prime\_hat: invetory

&nbsp;           L = (2\*np.max(s\_prime-s\_prime\_hat,0) + 1\*np.max(s\_prime\_hat-s\_prime,0)) / NM

&nbsp;           dL\_dhat = np.where(s\_prime-s\_prime\_hat>0, -2, 1)

&nbsp;       

&nbsp;       return L, dL\_dhat

&nbsp;   

&nbsp;   

&nbsp;   

&nbsp;   L, dL\_dhat = get\_Loss(s\_prime=s\_prime,s\_prime\_hat=s\_prime\_hat, Loss=Loss)

&nbsp;   

&nbsp;   ##Backpropagation    

&nbsp;   self.backpropagation(dL\_dhat)

&nbsp;   

&nbsp;   

&nbsp;   return np.mean(L)

&nbsp;                                             

&nbsp;       

def backpropagation(self, dL\_dhat):

&nbsp;   dhat\_dW = inputList\[self.Depth-1].T

&nbsp;   #

&nbsp;   presWeight = (self.identifier).T

&nbsp;   self.identifier -= LEARNING\_RATE\* dhat\_dW@dL\_dhat + self.Lambda\*self.identifier

&nbsp;   

&nbsp;   dL\_dout = (dL\_dhat\*1)@presWeight

&nbsp;   for i in range(0, self.Depth-1, -1):

&nbsp;       dout\_dW = inputList\[i].T #derivative\_Leack\_ReLU(inputList\[i+1]

&nbsp;       dout\_dB = 1\*derivative\_Leack\_ReLU(inputList\[i+1])

&nbsp;       #

&nbsp;       presWeight = self.weights\[i].T

&nbsp;       self.weights\[i] -= LEARNING\_RATE\* dout\_dW@(dL\_dout\*derivative\_Leack\_ReLU(inputList\[i+1])) + self.Lambda \* self.weights\[i]

&nbsp;       self.biases\[i] -=  LEARNING\_RATE\* dout\_dB\*(dL\_dout)

&nbsp;       

&nbsp;       dL\_dout= (dL\_dout\*derivative\_Leack\_ReLU(inputList\[i+1]))@presWeight

&nbsp;                                         

&nbsp;   return None



&nbsp;                                         



Identifier.forward=forward

Identifier.train=train

Identifier.backpropagation=backpropagation

&nbsp;                                       

```



Overflow maybe arise ,when the Learning rate is sufficiently high





```python

def f(x):

&nbsp;   return x\*\*2







x=np.linspace(-10,10,100)

y=f(x)





LEARNING\_RATE=0.000000001

identifier = Identifier(Dim\_in=1, Dim\_out=1, hiddens=\[64,128,256,64,32])



for \_ in range(10000):

&nbsp;   identifier.train(inputs=x, targets=y)

mse = identifier.train(inputs=x, targets=y)



print(mse)

y\_hat=identifier.forward(x)

plt.plot(x,y\_hat,c='b',label='idenfifier')

plt.plot(x,y,c='r',label='target')

plt.legend()

plt.show()

```



&nbsp;   0.7174388287505724

&nbsp;   





&nbsp;   

!\[png](/\_posts/picture/output\_5\_1.png)

&nbsp;   







```python

from scipy.stats import norm

from scipy.stats import ttest\_ind



def f(x):

&nbsp;   global w

&nbsp;   w=np.ones(10)/10

&nbsp;   y=np.random.normal(3+x@w)

&nbsp;   return y



costs=\[]

costs\_opt=\[]



for \_ in range(10):

&nbsp;   #demand data

&nbsp;   x=np.random.rand(100,10)

&nbsp;   y=f(x)



&nbsp;   #optimal cost

&nbsp;   y\_opt=norm.ppf(2/3, loc=3+x@w)

&nbsp;   cost\_opt= (2\*np.max(y-y\_opt,0) + 1\*np.max(y\_opt-y,0)) / 100

&nbsp;   costs\_opt.append(cost\_opt)

&nbsp;   

&nbsp;   #

&nbsp;   LEARNING\_RATE=0.000000001

&nbsp;   identifier = Identifier(Dim\_in=10, Dim\_out=1, hiddens=\[32,64,128,256,64,32])



&nbsp;   for \_ in range(10000):

&nbsp;       identifier.train(inputs=x, targets=y, Loss="Newsvendor")

&nbsp;   cost = identifier.train(inputs=x, targets=y, Loss="Newsvendor")

&nbsp;   costs.append(cost)



print(np.mean(costs))

print(np.mean(costs\_opt))



print(ttest\_ind(costs,costs\_opt,equal\_var=False))



plt.boxplot(\[costs,costs\_opt])

plt.xticks(\[1,2],\['deep','opt'])

plt.show()

```



&nbsp;   0.07442943501667457

&nbsp;   0.07062499773231493

&nbsp;   Ttest\_indResult(statistic=0.8206760282986177, pvalue=0.4239645263738895)

&nbsp;   





&nbsp;   

!\[png](/\_posts/picture/output\_5\_2.png)

&nbsp;   







```python



```



