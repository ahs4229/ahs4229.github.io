---
title: Multi-layerd Perceptron as identifier
categories: [code]
comments: true
---


```python
import numpy as np
import matplotlib.pyplot as plt
import random
from copy import deepcopy
```


```python
def Leaky_ReLU(x):
    return np.maximum(x,-0.001*x)

def derivative_Leack_ReLU(x):
    if(x<0):
        return -0.001
    else: 
        return 1
```


```python
LEARNING_RATE=0.00001

class Identifier:
    def __init__(self, Dim_in=2*2, Dim_out=2, hiddens=[128,128], l=0.001):
        '''
        Identifier: S x A â†’ S
        '''
        self.Lambda=l
        

        #Initialization
        self.Dim_in= Dim_in
        self.Dim_out= Dim_out
        self.hiddens = hiddens
        self.Depth= len(hiddens)+1
        
        
        self.weights= []
        self.biases= []
        
        #State X Action -> R^n
        inputSize = self.Dim_in 
        for nextSize in self.hiddens:
            self.weights.append( 2*np.random.rand(inputSize, nextSize) -1 )
            self.biases.append( 2*np.random.rand(1, nextSize) -1)
            inputSize= nextSize
            
        #Identifier Net: R^n -> State
        self.identifier= 2*np.random.rand(inputSize, self.Dim_out) -1

```


```python
def forward(self, inputs):
    inputs= np.array(inputs).reshape(-1,self.Dim_in)
    
    global inputList 
    inputList = [inputs]
    
    for i in range( len(self.hiddens) ):
        outputs = (inputs)@self.weights[i] + self.biases[i]
        inputs =  Leaky_ReLU(outputs)
        inputList.append(inputs)
    s_prime_hat = (inputs)@self.identifier   
    return s_prime_hat
 
    
def train(self, inputs=[], targets=[]):
    #Data= [s]+[a] 
    inputs= np.array(inputs).reshape(-1,self.Dim_in) 
    s_prime= np.array(targets).reshape(-1,self.Dim_out) 
    
    
    ##forward propagation
    s_prime_hat= self.forward(inputs)
    
    ##Loss calculation
    deviation= s_prime-s_prime_hat
    Loss= (deviation**2)/2/NM 
    #
    l2= np.sum(self.identifier)
    for weight in self.weights:
        l2+=np.sum(weight)
    l2 *= self.Lambda
    
    ##Backpropagation
    dL_dhat = -deviation/NM
    self.backpropagation(dL_dhat)
    
    
    return np.sum(Loss)
                                              
        
def backpropagation(self, dL_dhat):
    dhat_dW = inputList[self.Depth-1].T
    #
    presWeight = (self.identifier).T
    self.identifier -= LEARNING_RATE* dhat_dW@dL_dhat + self.Lambda*self.identifier
    
    dL_dout = (dL_dhat*1)@presWeight
    for i in range(0, self.Depth-1, -1):
        dout_dW = inputList[i].T #derivative_Leack_ReLU(inputList[i+1]
        dout_dB = 1*derivative_Leack_ReLU(inputList[i+1])
        #
        presWeight = self.weights[i].T
        self.weights[i] -= LEARNING_RATE* dout_dW@(dL_dout*derivative_Leack_ReLU(inputList[i+1])) + self.Lambda * self.weights[i]
        self.biases[i] -=  LEARNING_RATE* dout_dB*(dL_dout)
        
        dL_dout= (dL_dout*derivative_Leack_ReLU(inputList[i+1]))@presWeight
                                          
    return None

                                          

Identifier.forward=forward
Identifier.train=train
Identifier.backpropagation=backpropagation
                                        
```

Overflow maybe arise ,when the Learning rate is sufficiently high


```python
def f(x):
    return x**2



x=np.linspace(-10,10,100)
y=f(x)


LEARNING_RATE=0.000000001
identifier = Identifier(Dim_in=1, Dim_out=1, hiddens=[64,128,256,64,32])

for _ in range(10000):
    identifier.train(inputs=x, targets=y)
mse = identifier.train(inputs=x, targets=y)

print(mse)
y_hat=identifier.forward(x)
plt.plot(x,y_hat,c='b',label='idenfifier')
plt.plot(x,y,c='r',label='target')
plt.legend()
plt.show()
```

    65.802114547743
    


    
![png](/_posts/picture/output_5_1.png)
    



```python

```
